{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stemming.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jdOAW98S9le"
      },
      "outputs": [],
      "source": [
        "polarity_file = \"review_polarity.zip\"\n",
        "!unzip {polarity_file}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "neg_dir = './review_polarity/txt_sentoken/neg'\n",
        "pos_dir = './review_polarity/txt_sentoken/pos'\n",
        "\n"
      ],
      "metadata": {
        "id": "aLLAqI0AbIY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_train_test():\n",
        "  neg_reviews = sorted(glob.glob('./review_polarity/txt_sentoken/neg/*'))\n",
        "  pos_reviews = sorted(glob.glob('./review_polarity/txt_sentoken/pos/*'))\n",
        "  train = neg_reviews[:750]+pos_reviews[:750]\n",
        "  test = neg_reviews[750:]+pos_reviews[750:]\n",
        "  return train, test\n",
        "  \n"
      ],
      "metadata": {
        "id": "c6mTa2fn2_ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Llj9t1m0vnaU",
        "outputId": "515d8f30-dba1-4532-d814-0fcdc591d4eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = get_train_test()"
      ],
      "metadata": {
        "id": "SLv11JSw4I2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import *"
      ],
      "metadata": {
        "id": "oNneSB5JGIip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def porter_stem(words):\n",
        "  stemmer = PorterStemmer()\n",
        "  s = [stemmer.stem(w) for w in words]\n",
        "  return s"
      ],
      "metadata": {
        "id": "3Ay_EeTZF8g3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.classify.rte_classify import ne\n",
        "from string import punctuation\n",
        "from os import listdir\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def load_doc(filename):\n",
        "\tfile = open(filename, 'r',encoding='utf-8', errors = 'ignore')\n",
        "\ttext = file.read()\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "def clean_doc(doc):\n",
        "\ttokens = doc.split()\n",
        "\ttable = str.maketrans('', '', punctuation)\n",
        "\ttokens = [w.translate(table) for w in tokens]\n",
        "\ttokens = [word for word in tokens if word.isalpha()]\n",
        "\tstop_words = set(stopwords.words('english'))\n",
        "\ttokens = [w for w in tokens if not w in stop_words]\n",
        "\ttokens = [word for word in tokens if len(word) > 1]\n",
        "\treturn tokens\n",
        "\n",
        "def add_doc_to_vocab(filename, vocab):\n",
        "\tdoc = load_doc(filename)\n",
        "\ttokens = clean_doc(doc)\n",
        "\tvocab.update(tokens)\n",
        "\n",
        "def get_vocab(paths):\n",
        "  vocab = Counter()\n",
        "  for path in paths:\n",
        "    add_doc_to_vocab(path, vocab)\n",
        "  return vocab\n",
        "\n",
        "baseline_neg_vocab = get_vocab(train[0:750])\n",
        "baseline_pos_vocab = get_vocab(train[750:])\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kUPNPChozg-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import sklearn\n"
      ],
      "metadata": {
        "id": "Zd7vOipG2KUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizeMe(line):\n",
        "    line = line.split(\" \")\n",
        "    line = [word.translate(str.maketrans('', '', string.punctuation)) for word in line]\n",
        "    line = [word for word in line if word.isalpha()]\n",
        "    line = [word for word in line if len(word) > 1]\n",
        "    line = [word for word in line if word not in set(nltk.corpus.stopwords.words('english'))]\n",
        "    line = porter_stem(line)\n",
        "    return(line)"
      ],
      "metadata": {
        "id": "04tYIQ7i5vbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_vocab = []\n",
        "for hello in baseline_neg_vocab.keys():\n",
        "  all_vocab.append(hello)\n",
        "for hello in baseline_pos_vocab.keys():\n",
        "  all_vocab.append(hello)\n",
        "all_vocab = porter_stem(all_vocab)\n",
        "all_vocab = sorted(list(set(all_vocab)))\n"
      ],
      "metadata": {
        "id": "R7a8tFiq4K1G"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier"
      ],
      "metadata": {
        "id": "uDyfbL0l59fR"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n"
      ],
      "metadata": {
        "id": "de07uTvfO0Cu"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train_fit(clf):\n",
        "  v = TfidfVectorizer(input = 'filename', tokenizer=tokenizeMe,vocabulary=all_vocab)\n",
        "  X_train = v.fit_transform(train).toarray()\n",
        "  labels_train = [0]*750+[1]*750\n",
        "  clf.fit(X_train,labels_train)\n",
        "  X_test = v.fit_transform(test)\n",
        "  scores = clf.predict(X_test)\n",
        "  test_gt = [0]*250+[1]*250\n",
        "  f1 = f1_score(test_gt,scores, average=\"micro\")\n",
        "  prec = precision_score(test_gt,scores,average=\"micro\")\n",
        "  recall = recall_score(test_gt,scores,average=\"micro\")\n",
        "  acc = accuracy_score(test_gt,scores)\n",
        "\n",
        "  print(\"F1 Score -->\"+str(f1))\n",
        "  print(\"Precision -->\"+str(prec))\n",
        "  print(\"Recall -->\" +str(recall))\n",
        "  print(\"Accuracy -->\"+str(acc))\n",
        "\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "nTU9nXzF8vQE"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "vZVG707w9JIi"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "joZf9SpD_DpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "names = [\n",
        "    \"Nearest Neighbors\",\n",
        "    \"Decision Tree\",\n",
        "    \"Random Forest\",\n",
        "    \"Neural Net\",\n",
        "]\n",
        "\n",
        "classifiers = [\n",
        "    KNeighborsClassifier(3),\n",
        "    DecisionTreeClassifier(max_depth=5),\n",
        "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
        "    MLPClassifier(alpha=1, max_iter=1000)]"
      ],
      "metadata": {
        "id": "E4CrUk1G9T9H"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(4):\n",
        "  #SVM requires sparse data\n",
        "  \n",
        "  print(names[i])\n",
        "  train_fit(classifiers[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Fe6a9QZfLIW",
        "outputId": "6ac24dff-5d61-4e93-be02-1e4dacde162f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nearest Neighbors\n",
            "F1 Score -->0.656\n",
            "Precision -->0.656\n",
            "Recall -->0.656\n",
            "Accuracy -->0.656\n",
            "Decision Tree\n",
            "F1 Score -->0.67\n",
            "Precision -->0.67\n",
            "Recall -->0.67\n",
            "Accuracy -->0.67\n",
            "Random Forest\n",
            "F1 Score -->0.506\n",
            "Precision -->0.506\n",
            "Recall -->0.506\n",
            "Accuracy -->0.506\n",
            "Neural Net\n",
            "F1 Score -->0.864\n",
            "Precision -->0.864\n",
            "Recall -->0.864\n",
            "Accuracy -->0.864\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "def train_fit_svm(clf):\n",
        "  v = CountVectorizer(input = 'filename', tokenizer=tokenizeMe,vocabulary=all_vocab)\n",
        "  X_train = v.fit_transform(train)\n",
        "  labels_train = [0]*750+[1]*750\n",
        "  clf.fit(X_train,labels_train)\n",
        "  X_test = v.fit_transform(test)\n",
        "  scores = clf.predict(X_test)\n",
        "  test_gt = [0]*250+[1]*250\n",
        "  f1 = f1_score(test_gt,scores, average=\"micro\")\n",
        "  prec = precision_score(test_gt,scores,average=\"micro\")\n",
        "  recall = recall_score(test_gt,scores,average=\"micro\")\n",
        "  acc = accuracy_score(test_gt,scores)\n",
        "\n",
        "  print(\"F1 Score -->\"+str(f1))\n",
        "  print(\"Precision -->\"+str(prec))\n",
        "  print(\"Recall -->\" +str(recall))\n",
        "  print(\"Accuracy -->\" +str(acc))\n",
        "svm_names = [\"Linear SVM\",\"RBF SVM\"]\n",
        "svms = [SVC(kernel=\"linear\", C=0.025),SVC(gamma=2, C=1)]\n",
        "\n"
      ],
      "metadata": {
        "id": "7hzgJom1jMLz"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(2):\n",
        "  print(svm_names[i])\n",
        "  train_fit_svm(svms[i])"
      ],
      "metadata": {
        "id": "pU9XFkexi1Yf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebf0ef95-7245-4238-ec06-7ec45ab737b5"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear SVM\n",
            "F1 Score -->0.824\n",
            "Precision -->0.824\n",
            "Recall -->0.824\n",
            "Accuracy -->0.824\n",
            "RBF SVM\n",
            "F1 Score -->0.5\n",
            "Precision -->0.5\n",
            "Recall -->0.5\n",
            "Accuracy -->0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_fit_dense(clf):\n",
        "  v = TfidfVectorizer(input = 'filename', tokenizer=tokenizeMe,vocabulary=all_vocab)\n",
        "  X_train = v.fit_transform(train)\n",
        "  X_train = X_train.toarray()\n",
        "  labels_train = [0]*750+[1]*750\n",
        "  clf.fit(X_train,labels_train)\n",
        "  X_test = v.fit_transform(test)\n",
        "  X_test = X_test.toarray()\n",
        "  scores = clf.predict(X_test)\n",
        "  test_gt = [0]*250+[1]*250\n",
        "  f1 = f1_score(scores, test_gt,average=\"micro\")\n",
        "  prec = precision_score(scores, test_gt,average=\"micro\")\n",
        "  recall = recall_score(scores, test_gt,average=\"micro\")\n",
        "  acc = accuracy_score(scores,test_gt)\n",
        "\n",
        "  print(\"F1 Score -->\"+str(f1))\n",
        "  print(\"Precision -->\"+str(prec))\n",
        "  print(\"Recall -->\" +str(recall))\n",
        "  print(\"Accuracy -->\"+str(acc))\n"
      ],
      "metadata": {
        "id": "MVMnet3Myadh"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"QDA\")\n",
        "train_fit_dense(QuadraticDiscriminantAnalysis())\n",
        "print(\"Naive Bayes\")\n",
        "train_fit_dense(GaussianNB())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9G1STrsxGmG",
        "outputId": "4e383ad0-f515-4bce-a1aa-0ec01387876a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QDA\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score -->0.512\n",
            "Precision -->0.512\n",
            "Recall -->0.512\n",
            "Accuracy -->0.512\n",
            "Naive Bayes\n",
            "F1 Score -->0.614\n",
            "Precision -->0.614\n",
            "Recall -->0.614\n",
            "Accuracy -->0.614\n"
          ]
        }
      ]
    }
  ]
}