{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "negation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jdOAW98S9le"
      },
      "outputs": [],
      "source": [
        "polarity_file = \"review_polarity.zip\"\n",
        "!unzip {polarity_file}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "neg_dir = './review_polarity/txt_sentoken/neg'\n",
        "pos_dir = './review_polarity/txt_sentoken/pos'\n",
        "\n"
      ],
      "metadata": {
        "id": "aLLAqI0AbIY1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_train_test():\n",
        "  neg_reviews = sorted(glob.glob('./review_polarity/txt_sentoken/neg/*'))\n",
        "  pos_reviews = sorted(glob.glob('./review_polarity/txt_sentoken/pos/*'))\n",
        "  train = neg_reviews[:750]+pos_reviews[:750]\n",
        "  test = neg_reviews[750:]+pos_reviews[750:]\n",
        "  return train, test\n",
        "  \n"
      ],
      "metadata": {
        "id": "c6mTa2fn2_ss"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Llj9t1m0vnaU",
        "outputId": "52cb0d40-7b8e-4d40-9ee4-42b62190ed65"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = get_train_test()"
      ],
      "metadata": {
        "id": "SLv11JSw4I2u"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import *\n",
        "from nltk.sentiment import SentimentAnalyzer\n",
        "from nltk.sentiment.util import mark_negation\n"
      ],
      "metadata": {
        "id": "oNneSB5JGIip"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def porter_stem(words):\n",
        "  stemmer = PorterStemmer()\n",
        "  s = [stemmer.stem(w) for w in words]\n",
        "  return s"
      ],
      "metadata": {
        "id": "3Ay_EeTZF8g3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def include_negation(words):\n",
        "  sentim_analyzer = SentimentAnalyzer() \n",
        "  ci = [nltk.sentiment.util.mark_negation(doc) for doc in words]\n",
        "  return ci\n"
      ],
      "metadata": {
        "id": "68a7ezb9hBzN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.classify.rte_classify import ne\n",
        "from string import punctuation\n",
        "from os import listdir\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def load_doc(filename):\n",
        "\tfile = open(filename, 'r',encoding='utf-8', errors = 'ignore')\n",
        "\ttext = file.read()\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "def clean_doc(doc):\n",
        "\ttokens = doc.split()\n",
        "\ttable = str.maketrans('', '', punctuation)\n",
        "\ttokens = [w.translate(table) for w in tokens]\n",
        "\ttokens = [word for word in tokens if word.isalpha()]\n",
        "\tstop_words = set(stopwords.words('english'))\n",
        "\ttokens = [w for w in tokens if not w in stop_words]\n",
        "\ttokens = [word for word in tokens if len(word) > 1]\n",
        "\treturn tokens\n",
        "\n",
        "def add_doc_to_vocab(filename, vocab):\n",
        "\tdoc = load_doc(filename)\n",
        "\ttokens = clean_doc(doc)\n",
        "\tvocab.update(tokens)\n",
        "\n",
        "def get_vocab(paths):\n",
        "  vocab = Counter()\n",
        "  for path in paths:\n",
        "    add_doc_to_vocab(path, vocab)\n",
        "  return vocab\n",
        "\n",
        "baseline_neg_vocab = get_vocab(train[0:750])\n",
        "baseline_pos_vocab = get_vocab(train[750:])\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kUPNPChozg-8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import sklearn\n"
      ],
      "metadata": {
        "id": "Zd7vOipG2KUi"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizeMe(line):\n",
        "    line = line.split(\" \")\n",
        "    line = [word.translate(str.maketrans('', '', string.punctuation)) for word in line]\n",
        "    line = [word for word in line if word.isalpha()]\n",
        "    line = [word for word in line if len(word) > 1]\n",
        "    line = [word for word in line if word not in set(nltk.corpus.stopwords.words('english'))]\n",
        "    line = porter_stem(line)\n",
        "    line = include_negation(line)\n",
        "    return(line)"
      ],
      "metadata": {
        "id": "04tYIQ7i5vbW"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_vocab = []\n",
        "for hello in baseline_neg_vocab.keys():\n",
        "  all_vocab.append(hello)\n",
        "for hello in baseline_pos_vocab.keys():\n",
        "  all_vocab.append(hello)\n",
        "all_vocab = porter_stem(all_vocab)\n",
        "all_vocab = include_negation(all_vocab)\n",
        "all_vocab = sorted(list(set(all_vocab)))\n"
      ],
      "metadata": {
        "id": "R7a8tFiq4K1G"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n"
      ],
      "metadata": {
        "id": "de07uTvfO0Cu"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train_fit(clf):\n",
        "  v = TfidfVectorizer(input = 'filename', tokenizer=tokenizeMe,vocabulary=all_vocab)\n",
        "  X_train = v.fit_transform(train).toarray()\n",
        "  labels_train = [0]*750+[1]*750\n",
        "  clf.fit(X_train,labels_train)\n",
        "  X_test = v.fit_transform(test)\n",
        "  scores = clf.predict(X_test)\n",
        "  test_gt = [0]*250+[1]*250\n",
        "  f1 = f1_score(test_gt,scores, average=\"micro\")\n",
        "  prec = precision_score(test_gt,scores,average=\"micro\")\n",
        "  recall = recall_score(test_gt,scores,average=\"micro\")\n",
        "  acc = accuracy_score(test_gt,scores)\n",
        "\n",
        "  print(\"F1 Score -->\"+str(f1))\n",
        "  print(\"Precision -->\"+str(prec))\n",
        "  print(\"Recall -->\" +str(recall))\n",
        "  print(\"Accuracy -->\"+str(acc))\n",
        "\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "nTU9nXzF8vQE"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.gaussian_process import GaussianProcessClassifier\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "vZVG707w9JIi"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "names = [\n",
        "    \"Nearest Neighbors\",\n",
        "    \"Decision Tree\",\n",
        "    \"Random Forest\",\n",
        "    \"Neural Net\",\n",
        "]\n",
        "\n",
        "classifiers = [\n",
        "    KNeighborsClassifier(3),\n",
        "    DecisionTreeClassifier(max_depth=5),\n",
        "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
        "    MLPClassifier(alpha=1, max_iter=1000)]"
      ],
      "metadata": {
        "id": "E4CrUk1G9T9H"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(4):\n",
        "  #SVM requires sparse data\n",
        "  \n",
        "  print(names[i])\n",
        "  train_fit(classifiers[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Fe6a9QZfLIW",
        "outputId": "a81f2cf8-0e46-44c1-c94f-af8b034b6295"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nearest Neighbors\n",
            "F1 Score -->0.656\n",
            "Precision -->0.656\n",
            "Recall -->0.656\n",
            "Accuracy -->0.656\n",
            "Decision Tree\n",
            "F1 Score -->0.674\n",
            "Precision -->0.674\n",
            "Recall -->0.674\n",
            "Accuracy -->0.674\n",
            "Random Forest\n",
            "F1 Score -->0.558\n",
            "Precision -->0.558\n",
            "Recall -->0.558\n",
            "Accuracy -->0.558\n",
            "Neural Net\n",
            "F1 Score -->0.866\n",
            "Precision -->0.866\n",
            "Recall -->0.866\n",
            "Accuracy -->0.866\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "def train_fit_svm(clf):\n",
        "  v = CountVectorizer(input = 'filename', tokenizer=tokenizeMe,vocabulary=all_vocab)\n",
        "  X_train = v.fit_transform(train)\n",
        "  labels_train = [0]*750+[1]*750\n",
        "  clf.fit(X_train,labels_train)\n",
        "  X_test = v.fit_transform(test)\n",
        "  scores = clf.predict(X_test)\n",
        "  test_gt = [0]*250+[1]*250\n",
        "  f1 = f1_score(test_gt,scores, average=\"micro\")\n",
        "  prec = precision_score(test_gt,scores,average=\"micro\")\n",
        "  recall = recall_score(test_gt,scores,average=\"micro\")\n",
        "  acc = accuracy_score(test_gt,scores)\n",
        "\n",
        "  print(\"F1 Score -->\"+str(f1))\n",
        "  print(\"Precision -->\"+str(prec))\n",
        "  print(\"Recall -->\" +str(recall))\n",
        "  print(\"Accuracy -->\" +str(acc))\n",
        "svm_names = [\"Linear SVM\",\"RBF SVM\"]\n",
        "svms = [SVC(kernel=\"linear\", C=0.025),SVC(gamma=2, C=1)]\n",
        "\n"
      ],
      "metadata": {
        "id": "7hzgJom1jMLz"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(2):\n",
        "  print(svm_names[i])\n",
        "  train_fit_svm(svms[i])"
      ],
      "metadata": {
        "id": "pU9XFkexi1Yf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b9c8c25-6bc6-4159-f50b-f7f04155fc27"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear SVM\n",
            "F1 Score -->0.824\n",
            "Precision -->0.824\n",
            "Recall -->0.824\n",
            "Accuracy -->0.824\n",
            "RBF SVM\n",
            "F1 Score -->0.5\n",
            "Precision -->0.5\n",
            "Recall -->0.5\n",
            "Accuracy -->0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_fit_dense(clf):\n",
        "  v = TfidfVectorizer(input = 'filename', tokenizer=tokenizeMe,vocabulary=all_vocab)\n",
        "  X_train = v.fit_transform(train)\n",
        "  X_train = X_train.toarray()\n",
        "  labels_train = [0]*750+[1]*750\n",
        "  clf.fit(X_train,labels_train)\n",
        "  X_test = v.fit_transform(test)\n",
        "  X_test = X_test.toarray()\n",
        "  scores = clf.predict(X_test)\n",
        "  test_gt = [0]*250+[1]*250\n",
        "  f1 = f1_score(scores, test_gt,average=\"micro\")\n",
        "  prec = precision_score(scores, test_gt,average=\"micro\")\n",
        "  recall = recall_score(scores, test_gt,average=\"micro\")\n",
        "  acc = accuracy_score(scores,test_gt)\n",
        "\n",
        "  print(\"F1 Score -->\"+str(f1))\n",
        "  print(\"Precision -->\"+str(prec))\n",
        "  print(\"Recall -->\" +str(recall))\n",
        "  print(\"Accuracy -->\"+str(acc))\n"
      ],
      "metadata": {
        "id": "MVMnet3Myadh"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"QDA\")\n",
        "train_fit_dense(QuadraticDiscriminantAnalysis())\n",
        "print(\"Naive Bayes\")\n",
        "train_fit_dense(GaussianNB())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9G1STrsxGmG",
        "outputId": "7218a7c3-a545-48a5-ad82-f0a843a6c310"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QDA\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/discriminant_analysis.py:878: UserWarning: Variables are collinear\n",
            "  warnings.warn(\"Variables are collinear\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score -->0.512\n",
            "Precision -->0.512\n",
            "Recall -->0.512\n",
            "Accuracy -->0.512\n",
            "Naive Bayes\n",
            "F1 Score -->0.614\n",
            "Precision -->0.614\n",
            "Recall -->0.614\n",
            "Accuracy -->0.614\n"
          ]
        }
      ]
    }
  ]
}